<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:40:16 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-24/HBASE-24.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-24] Scaling: Too many open file handles to datanodes</title>
                <link>https://issues.apache.org/jira/browse/HBASE-24</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We&apos;ve been here before (&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-2341&quot; title=&quot;Datanode active connections never returns to 0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-2341&quot;&gt;&lt;del&gt;HADOOP-2341&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Today the rapleaf gave me an lsof listing from a regionserver.  Had thousands of open sockets to datanodes all in ESTABLISHED and CLOSE_WAIT state.  On average they seem to have about ten file descriptors/sockets open per region (They have 3 column families IIRC.  Per family, can have between 1-5 or so mapfiles open per family &amp;#8211; 3 is max... but compacting we open a new one, etc.).&lt;/p&gt;

&lt;p&gt;They have thousands of regions.   400 regions &amp;#8211; ~100G, which is not that much &amp;#8211; takes about 4k open file handles.&lt;/p&gt;

&lt;p&gt;If they want a regionserver to server a decent disk worths &amp;#8211; 300-400G &amp;#8211; then thats maybe 1600 regions... 16k file handles.  If more than just 3 column families..... then we are in danger of blowing out limits if they are 32k.&lt;/p&gt;

&lt;p&gt;We&apos;ve been here before with &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-2341&quot; title=&quot;Datanode active connections never returns to 0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-2341&quot;&gt;&lt;del&gt;HADOOP-2341&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A dfsclient that used non-blocking i/o would help applications like hbase (The datanode doesn&apos;t have this problem as bad &amp;#8211; CLOSE_WAIT on regionserver side, the bulk of the open fds in the rapleaf log, don&apos;t have a corresponding open resource on datanode end).&lt;/p&gt;

&lt;p&gt;Could also just open mapfiles as needed, but that&apos;d kill our random read performance and its bad enough already.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12386027">HBASE-24</key>
            <summary>Scaling: Too many open file handles to datanodes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Jan 2008 06:48:53 +0000</created>
                <updated>Thu, 18 Feb 2016 22:22:32 +0000</updated>
                            <resolved>Thu, 18 Feb 2016 22:22:32 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>20</watches>
                                                                <comments>
                            <comment id="12558268" author="cwalters" created="Sat, 12 Jan 2008 17:40:51 +0000"  >&lt;p&gt;Perhaps we should just open the mapfiles as needed and measure the effect on random reads. It could turn out that the effect is not that big, given that random reads are not performing well anyway. Focusing on allowing folks to scale up rather than performance on the random read use case seems like it could be the right trade-off at the moment and give us some breathing room to address the non-blocking IO issue at a more measured pace.&lt;/p&gt;</comment>
                            <comment id="12558296" author="stack" created="Sat, 12 Jan 2008 21:23:29 +0000"  >&lt;p&gt;We could do that or make it optional behavior until we figure a fix.&lt;/p&gt;

&lt;p&gt;FYI, to do a cold-start random read into a MapFile, you need to open two files (the data file and its index), read all of the index into memory, find the closest offset in the index and then seek around in the data file to find the asked-for key.  In hbase currently, only the data file is held open (the index file read into memory has been forced and the index file has then been let go).&lt;/p&gt;</comment>
                            <comment id="12565953" author="jimk" created="Tue, 5 Feb 2008 23:20:39 +0000"  >&lt;p&gt;Changing priority to Critical to emphasize that this is one of the major roadblocks to scalability that we have.&lt;/p&gt;

&lt;p&gt;And the problem is not only on the number of connections the region servers have open to the dfs, but also include datanode connections for each open file.&lt;/p&gt;</comment>
                            <comment id="12589768" author="bryanduxbury" created="Wed, 16 Apr 2008 23:27:05 +0000"  >&lt;p&gt;This is really more of a DFS issue, and there&apos;s an admittedly suboptimal solution to be had in increasing the max number of open file handles at the OS level. As such, we&apos;re going to hold off on solving this issue until after 0.2.&lt;/p&gt;</comment>
                            <comment id="12613082" author="ln@webcate.net" created="Sat, 12 Jul 2008 13:01:09 +0000"  >&lt;p&gt;from my profiling result, memory usage of a regionserver is determined by 2 things:&lt;br/&gt;
1. the mapfile index read into memory(io.map.index.skip can adjust it, buf allwill stay in mem weather u need it or not)&lt;br/&gt;
2. data output buffer used by each SequenceFile$Reader(each can measured as the largest value size in the file)&lt;br/&gt;
3. memcache, can controlled by hbase itsself. &lt;/p&gt;

&lt;p&gt;so, when concurrent open mapfiles limited,  memory usage of a regionserver limited too.  otherwise, a regionserver will cause OOME when data size increasing. in my test env, 100G data(200M mapfile index total, 2000 HStoreFiles opened, 512M mamcache) used 1.5G heap memory, the max i can set using -Xmx.&lt;/p&gt;

&lt;p&gt;i think this issue should be seriously concerned by Hbase, not only DFS side.&lt;/p&gt;</comment>
                            <comment id="12613104" author="stack" created="Sat, 12 Jul 2008 17:15:11 +0000"  >&lt;p&gt;Thanks for doing the profiling LN.  Do you think we should put an upper bound on the number of regions a particular regionserver can carry at any one time or an upper bound on number of open Readers?  I wonder, if you want to carry many regions, if lowering the compaction threshold from 3 to 2 &amp;#8211; or even to 1 &amp;#8211; would make any difference in our memory profile (at a CPU cost)?  We load the index and keep it around to avoid doing it on each random access &amp;#8211; maybe if a bounded pool of open MapFiles, we could move files in and out of the pool on some kind of LRU basis?&lt;/p&gt;</comment>
                            <comment id="12613567" author="ln@webcate.net" created="Tue, 15 Jul 2008 09:38:52 +0000"  >&lt;p&gt;i have file a new issue &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-745&quot; title=&quot;scaling of one regionserver, improving memory and cpu usage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-745&quot;&gt;&lt;del&gt;HBASE-745&lt;/del&gt;&lt;/a&gt; for scalability discussion of regionserver.&lt;br/&gt;
btw, i don&apos;t think reducing threshold is a good idea, because it will waste cpu time for compaction, described in that issue.&lt;/p&gt;</comment>
                            <comment id="12614919" author="apurtell" created="Sat, 19 Jul 2008 00:12:46 +0000"  >&lt;p&gt;I think the bounded pool idea, dynamically sized to the available heap on startup, makes a lot of sense. &lt;/p&gt;

&lt;p&gt;Also I wonder if region servers can/should relinquish or reject regions after a threshold of available to maximum heap is reached and forced GC does not help? &lt;/p&gt;</comment>
                            <comment id="12614928" author="viper799" created="Sat, 19 Jul 2008 00:56:54 +0000"  >&lt;p&gt;We may not need to reject if we do a little more work on the load balancing  functions and make them smarter by factoring in memory size&lt;/p&gt;</comment>
                            <comment id="12641157" author="stack" created="Mon, 20 Oct 2008 21:15:52 +0000"  >&lt;p&gt;Putting into 0.19.  We can move it out later.&lt;/p&gt;</comment>
                            <comment id="12641640" author="apurtell" created="Tue, 21 Oct 2008 22:31:12 +0000"  >&lt;p&gt;This came up on IRC today.&lt;/p&gt;</comment>
                            <comment id="12641948" author="stack" created="Wed, 22 Oct 2008 19:12:19 +0000"  >&lt;p&gt;Lets play with all ideas suggested above &amp;#8211; only open on demand, an LRU/ARU&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; cache of opened mapfiles, etc.  I don&apos;t think it&apos;d be hard to do and it would get us over this awkward hump in our scaling story.&lt;/p&gt;</comment>
                            <comment id="12662570" author="stack" created="Sat, 10 Jan 2009 00:21:19 +0000"  >&lt;p&gt;Here are related remarks made by Jean-Adrien over in hadoop.  Baseline is that we&apos;ve been bandaging over this uglyness for a while now.  Time to address the puss.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
xceiverCount limit reason
Click to flag &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; post

by Jean-Adrien Jan 08, 2009; 03:03am :: Rate &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; Message: - Use ratings to moderate (?)

Reply | Reply to Author | Print | View Threaded | Show Only &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; Message
Hello all,

I&apos;m running HBase on top of hadoop and I have some difficulties to tune hadoop conf in order to work fine with HBase.
My configuration is 4 desktop class machines, 2 are running a datanode/region server, 1 only a region server and 1 a namenode/hbase master, 1Gb RAM each

When I start HBase, about 300 regions must be load on 3 region servers; a lot of accesses are made concurrently on Hadoop. My first problem, using the &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; configuration, was to see too many of:
DataXceiver: java.net.SocketTimeoutException: 480000 millis timeout &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; channel to be ready &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; write.

I was wondering what the reason of such a time out is. Where is the bottleneck ? First I believed that was a network problem (I have  100Mbits/s interfaces). But after monitoring the network, it seems the load is low when it happens.
Anyway, I found the parameter
dfs.datanode.socket.write.timeout and I set it 0 to disable the timeout.

Then I saw in datanodes
xceiverCount 256 exceeds the limit of concurrent xcievers 255
What is exactly the role of the receivers ? to receive the replicated blocks and/or to receive the file from clients ?
When their threads end ? When their threads are created ?

Anyway, I found the parameter
dfs.datanode.max.xcievers
I upped it to 511, then to 1023 and today to 2047; but by cluster is not so big (300 HBase regions, 200Gb including replication factor of 2); I&apos;m not sure I will be able to up &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; limit &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; time. Moreover, it considerably increases the amount of virtual memory needed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the datanode jvm (about 2Gb now, only 500Mb &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; heap). That yields to excessive swap, and a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; problem arises; some leases expired, and my entire cluster eventually fails.

Can I tune other parameter to avoid these concurrent receivers to be created ?
Upping the dfs.replication.interval &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; example could help ?

Could the fact the I run the regionserver on the same machine that the datanode up the amount of xciever ? in which &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; I&apos;ll &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; a different layout, and use the network bottleneck to avoid stress datanodes.

Any clue on the inside-hadoop-xciever would be appreciated.
Thanks.

-- Jean-Adrien
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;... and&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Some more information about the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt;.

I read the HADOOP-3633 / 3859 / 3831 in jira.
I run the version 18.1 of hadoop therefore I have no fix &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 3831.
Nevertheless my problem seems different.
The threads are created as soon the client (HBase) requests data. the data
arrives to HBase without problem but the thread never ends. Looking at the #
of threads graphs:

http:&lt;span class=&quot;code-comment&quot;&gt;//www.nabble.com/file/p21352818/launch_tests.png
&lt;/span&gt;(you might need to go to nabble to see the image:
http:&lt;span class=&quot;code-comment&quot;&gt;//www.nabble.com/xceiverCount-limit-reason-tp21349807p21349807.html
&lt;/span&gt;
In the graph one runs hadoop / HBase 3 times (A/B/C) :
A:
I configure hadoop with dfs.datanode.max.xcievers=2023 and
dfs.datanode.socket.write.timeout=0
as soon I start hbase, the region load their data from dfs and the number of
threads climbs up to 1100 in about 2-3 min. Then it stays in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; scope.
All DataXceiver threads are in one of these two states:

&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.hadoop.dfs.DataNode$DataXceiver@6a2f81&quot;&lt;/span&gt; daemon prio=10
tid=0x08289c00 nid=0x6bb6 runnable [0x8f980000..0x8f981140]
  java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
       at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
       at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
       at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
       - locked &amp;lt;0x95838858&amp;gt; (a sun.nio.ch.Util$1)
       - locked &amp;lt;0x95838868&amp;gt; (a java.util.Collections$UnmodifiableSet)
       - locked &amp;lt;0x95838818&amp;gt; (a sun.nio.ch.EPollSelectorImpl)
       at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
       at
org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:260)
       at
org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:155)
       at
org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:150)
       at
org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:123)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
       - locked &amp;lt;0x95838b90&amp;gt; (a java.io.BufferedInputStream)
       at java.io.DataInputStream.readShort(DataInputStream.java:295)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1115)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1037)
       at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:619)

&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.hadoop.dfs.DataNode$DataXceiver@1abf87e&quot;&lt;/span&gt; daemon prio=10
tid=0x90bbd400 nid=0x61ae runnable [0x7b68a000..0x7b68afc0]
  java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
       at java.net.SocketInputStream.socketRead0(Native Method)
       at java.net.SocketInputStream.read(SocketInputStream.java:129)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
       - locked &amp;lt;0x9671a8e0&amp;gt; (a java.io.BufferedInputStream)
       at java.io.DataInputStream.readShort(DataInputStream.java:295)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1115)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1037)
       at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:619)


B:
I changed hadoop configuration, introducing the &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; 8min timeout.
Once again, as soon HBase gets data from dfs, the number of thread grows to
1100. After 8 minutes the timeout fires, and they fail one after each other
with the exception:

2009-01-08 14:21:09,305 WARN org.apache.hadoop.dfs.DataNode:
DatanodeRegistration(192.168.1.13:50010,
storageID=DS-1681396969-127.0.1.1-50010-1227536709605, infoPort=50075,
ipcPor
t=50020):Got exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; serving blk_-1718199459793984230_722338 to
/192.168.1.13:
java.net.SocketTimeoutException: 480000 millis timeout &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;
channel to be ready &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; write. ch :
java.nio.channels.SocketChannel[connected local=/192.168.1.13:50010 re
mote=/192.168.1.13:37462]
       at
org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)
       at
org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)
       at
org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)
       at
org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1873)
       at
org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1967)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1109)
       at
org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1037)
       at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:619)

C:
During &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; third session, I made the same run, but before the timeout
fires, I stop HBase. In &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt;, the thread ends correctly.

Is it the responsibility of hadoop client too manage its connection pool
with the server ? In which &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; the problem would be an HBase problem?
Anyway I found my problem, it is not a matter of performances.

Thanks &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your answers
Have a nice day.

-- Jean-Adrien
--
View &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; message in context: http:&lt;span class=&quot;code-comment&quot;&gt;//www.nabble.com/xceiverCount-limit-reason-tp21349807p21352818.html
&lt;/span&gt;- Show quoted text -
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And Raghu:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Jean-Adrien wrote:

    Is it the responsibility of hadoop client too manage its connection pool
    with the server ? In which &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; the problem would be an HBase problem?
    Anyway I found my problem, it is not a matter of performances.


Essentially, yes. Client has to close the file to relinquish connections, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; clients are using the common read/write &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt;.

Currently &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; a client keeps many hdfs files open, it results in many threads held at the DataNodes. As you noticed, timeout at DNs helps.

Various solutions are possible at different levels: application(hbase), Client API, HDFS, etc. https:&lt;span class=&quot;code-comment&quot;&gt;//issues.apache.org/jira/browse/HADOOP-3856 is proposal at HDFS level.&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12662572" author="stack" created="Sat, 10 Jan 2009 00:22:07 +0000"  >&lt;p&gt;Made it a blocker.  Fellas have been running into this in various guises; e.g. jgray recent OOME in DNs.&lt;/p&gt;</comment>
                            <comment id="12662728" author="ln@webcate.net" created="Sun, 11 Jan 2009 06:09:13 +0000"  >&lt;p&gt;sharing my experience here, hope it helpful:&lt;br/&gt;
1. since hbase never close mapfiles in normal usage, timeout of dfs.datanode.socket.write.timeout should always be set.&lt;br/&gt;
2. xceiverCount should be set too, or there will be a stack memory overflow or reaching other resource limit.&lt;br/&gt;
3. as my comments above, the only solution is limiting hbase concurrent open mapfiles, or it will cause OOME as data size increasing.&lt;/p&gt;

&lt;p&gt;posting my patch here:&lt;br/&gt;
1. this patch made for hbase 0.18.0, and it works well in last 3 month, about 500G data in 4 machines now;&lt;br/&gt;
2. the patch make HStoreFile..HbaseReader extends MonitoredReader instead of the original MapFile.Reader, so we can control things in it;&lt;br/&gt;
3. see javadoc of MonitoredReader for more concurrent open controlling detail.&lt;/p&gt;</comment>
                            <comment id="12662729" author="ln@webcate.net" created="Sun, 11 Jan 2009 06:11:34 +0000"  >&lt;p&gt;the patch for HStoreFile.java&lt;/p&gt;</comment>
                            <comment id="12662732" author="stack" created="Sun, 11 Jan 2009 06:31:31 +0000"  >&lt;p&gt;Hey Luo Ning:&lt;/p&gt;

&lt;p&gt;So I understand correctly, you are saying that rather than a timeout of 0 for dfs.datanode.socket.write.timeout as we suggest elsewhere, you say that it should be set say to the current default of 8 minutes?&lt;/p&gt;

&lt;p&gt;Also, rather than keep raising the xceiverCount as more regions are added, you&apos;re saying that it should be bounded &amp;#8211; at the current default of 256? &amp;#8211; otherwise the OOME about can&apos;t create thread, etc?&lt;/p&gt;

&lt;p&gt;How many regions per server with your patch in place Luo Ning?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="12662736" author="ln@webcate.net" created="Sun, 11 Jan 2009 07:22:28 +0000"  >&lt;p&gt;my understands:&lt;br/&gt;
xceiverCount must be set to avoiding OOME, with -Xss option and memory size a java process can use, it can be set more larger than 256, i&apos;m using 2500 in my DN.&lt;/p&gt;

&lt;p&gt;how may concurrent open mapfiles in hbase, how may xceiver thread in hadoop DNs. &lt;/p&gt;

&lt;p&gt;if set write.timeout to 0, xceiverCount will reach soon, as hbase data increasing, then DN hangs.&lt;/p&gt;

&lt;p&gt;set write.timeout to 8 min, only useful in following situation: hbase never read/write more the &apos;xceiverCount&apos; mapfiles  in the 8 min, otherwise the DN will stop serving data.&lt;/p&gt;

&lt;p&gt;the chains is: more data -&amp;gt; more mapfiles -&amp;gt;more opening mapfiles -&amp;gt; more xceiver threads, comprare all solutions we can have in the chain, controlling opening mapfiles is the most efficient.&lt;/p&gt;

&lt;p&gt;my hbase installation: 2 regionservers/namenodes, each handle about 500 region, 250G data. concurrent open mapfiles set to 2000, and xceiverCount should larger than it, so set to 2500.&lt;/p&gt;
</comment>
                            <comment id="12662740" author="stack" created="Sun, 11 Jan 2009 07:52:35 +0000"  >&lt;p&gt;Thank Luo Ning.  Yeah, agree with your exposition above.  In 0.19.0 hadoop, I believe the timed-out socket on datanode is revived by dfsclient (hadoop-3831).  I need to test.  Regardless, we need to put a bound on number of mapfiles (As you&apos;ve been saying for a good while now).  Let me look at your patch (500 regions per server is really good for 0.18.x hbase).&lt;/p&gt;</comment>
                            <comment id="12701774" author="stack" created="Thu, 23 Apr 2009 00:59:56 +0000"  >&lt;p&gt;Moving out of 0.20.0 &amp;#8211; we won&apos;t get to it I&apos;m thinking.&lt;/p&gt;</comment>
                            <comment id="12867029" author="stack" created="Thu, 13 May 2010 04:42:06 +0000"  >&lt;p&gt;Moved from 0.21 to 0.22 just after merge of old 0.20 branch into TRUNK.&lt;/p&gt;</comment>
                            <comment id="13039507" author="yuzhihong@gmail.com" created="Thu, 26 May 2011 04:23:56 +0000"  >&lt;p&gt;Should we take further action on this JIRA ?&lt;/p&gt;</comment>
                            <comment id="13047504" author="stack" created="Fri, 10 Jun 2011 22:18:57 +0000"  >&lt;p&gt;Moving out of 0.92.0.&lt;/p&gt;</comment>
                            <comment id="13089042" author="posix4e" created="Mon, 22 Aug 2011 21:40:24 +0000"  >&lt;p&gt;+            conf); // defer opening streams&lt;br/&gt;
is in the patch. Are we actually still defering. That seems wrong.&lt;/p&gt;</comment>
                            <comment id="14267759" author="clehene" created="Wed, 7 Jan 2015 15:34:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; this is a venerable one from the oldies but goldies series &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; as we long ago learnt to up xceivers max open file handles and the like, we haven&apos;t been affected, however I&apos;m curious if this is something that would be worth a refresh.&lt;/p&gt;</comment>
                            <comment id="15153236" author="stack" created="Thu, 18 Feb 2016 22:22:32 +0000"  >&lt;p&gt;Resolving as &apos;wont fix&apos;... We open lots of files. Would be good if we opened less.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12467373">HBASE-2751</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12397639" name="HBASE-823.patch" size="1344" author="ln@webcate.net" created="Sun, 11 Jan 2009 06:11:34 +0000"/>
                            <attachment id="12397640" name="MonitoredReader.java" size="10262" author="ln@webcate.net" created="Sun, 11 Jan 2009 06:12:35 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 12 Jan 2008 17:40:51 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24862</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            43 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02c8n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11584</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>