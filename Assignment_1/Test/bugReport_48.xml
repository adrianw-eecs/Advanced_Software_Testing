<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:40:30 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-48/HBASE-48.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-48] [hbase] Bulk load tools</title>
                <link>https://issues.apache.org/jira/browse/HBASE-48</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Hbase needs tools to facilitate bulk upload and possibly dumping.  Going via the current APIs, particularly if the dataset is large and cell content is small, uploads can take a long time even when using many concurrent clients.&lt;/p&gt;

&lt;p&gt;PNUTS folks talked of need for a different API to manage bulk upload/dump.&lt;/p&gt;

&lt;p&gt;Another notion would be to somehow have the bulk loader tools somehow write regions directly in hdfs.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12380704">HBASE-48</key>
            <summary>[hbase] Bulk load tools</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Oct 2007 18:32:30 +0000</created>
                <updated>Fri, 20 Nov 2015 13:01:21 +0000</updated>
                            <resolved>Wed, 23 Sep 2009 18:43:40 +0000</resolved>
                                    <version>0.20.0</version>
                                    <fixVersion>0.20.1</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>5</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="12540582" author="stack" created="Tue, 6 Nov 2007 21:40:06 +0000"  >&lt;p&gt;Bulk uploader needs to be able to tolerate myriad data input types.  Data will likely need massaging and ultimately, if writing HRegion content directly into HDFS rather than going against hbase API &amp;#8211; preferred since it&apos;ll be dog slow doing bulk uploads going against hbase API &amp;#8211; then it has to be sorted.  Using mapreduce would make sense.&lt;/p&gt;

&lt;p&gt;Look too at using PIG because it has a few LOAD implementations &amp;#8211; from files on local or HDFS &amp;#8211; and some facility for doing transforms on data moving tuples around.  Would need to write a special STORE operator that wrote the data sorted out as HRegions direct into HDFS (This would be different than &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-6&quot; title=&quot;Addition of Hbase Storage Option In Load/Store Statement&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-6&quot;&gt;&lt;del&gt;PIG-6&lt;/del&gt;&lt;/a&gt; which is about writing into hbase via API).&lt;/p&gt;

&lt;p&gt;Also, chatting with Jim, this is a pretty important issue.  This is the first folks run into when they start to get serious about hbase.&lt;/p&gt;</comment>
                            <comment id="12549661" author="bryanduxbury" created="Sat, 8 Dec 2007 06:14:55 +0000"  >&lt;p&gt;A really cool feature for bulk loading would be artificially lowering the split size so that splits occur really often, at least until there are as many regions as there are regionservers. That way, the load operation could have a lot more parallelism early on. &lt;/p&gt;</comment>
                            <comment id="12549753" author="cwalters" created="Sat, 8 Dec 2007 21:35:18 +0000"  >&lt;p&gt;I like the idea of lots of splits early on when the number of regions is less than the number of region servers. You want to make sure the splits are made at points that relatively well-distributed, of course, so don&apos;t make it so small that you split without a representative sampling. This would be a good general purpose solution that doesn&apos;t create a new API. Then the bulk upload simply looks like partitioning the data set and uploading via Map-Reduce, perhaps with batched inserts. Do you really think this would be dog slow?&lt;/p&gt;

&lt;p&gt;If that is not fast enough, I suppose we could have a mapfile uploader. This would require the dataset to be prepared properly, which could be a bit fidgety (sorting, properly splitting across columns, etc.).&lt;/p&gt;</comment>
                            <comment id="12549765" author="bryanduxbury" created="Sat, 8 Dec 2007 23:52:11 +0000"  >&lt;p&gt;Actually you wouldn&apos;t have to be too concerned with the distribution of splits early on, because even if some of the regions ended up being abnormally small, they would eventually be merged with neighboring regions, no?&lt;/p&gt;</comment>
                            <comment id="12549785" author="cwalters" created="Sun, 9 Dec 2007 01:44:10 +0000"  >&lt;p&gt;Eventually that might be true but merging is currently a manually-triggered operation. Also, unless a more intelligent heuristic were in place, a small region would count against a whole region server until it was merged, which would slow down the loading.&lt;/p&gt;</comment>
                            <comment id="12566338" author="viper799" created="Wed, 6 Feb 2008 21:07:12 +0000"  >&lt;p&gt;Would not the best way to do this would be to do a map that formats and sorts the data per column family then a reduce that writes a mapfiles directly to the regions columns?&lt;/p&gt;

&lt;p&gt;Then that would skip the api and speed up the loading of the data and it would not matter so much if we has 1 region or not sense all we would be doing is adding a mapfile to hdfs.&lt;br/&gt;
Course the map would have to know if there is 1 region or 1000 and split the data correctly but even if each map &lt;br/&gt;
only produces a few lines of data per column family the compactor will come along sooner or later and clean up and split where needed.&lt;/p&gt;

&lt;p&gt;So if we add 100 map files to one column I would assume that it would slow reads down a little bit havening to sort threw all the map files while scanning but that would be a temporary speed problem.&lt;/p&gt;</comment>
                            <comment id="12566394" author="stack" created="Wed, 6 Feb 2008 23:19:24 +0000"  >&lt;p&gt;Yes.  Going behind the API would be the faster way to load hbase.   It&apos;d be dangerous to do into a live hbase.   Should we write something like TableOutputFormatter except it writes region files directly into hdfs?   It&apos;d make a region per reducer instance?  It&apos;d know how to write keys, etc. properly and what location in hdfs to place files.&lt;/p&gt;</comment>
                            <comment id="12566397" author="bryanduxbury" created="Wed, 6 Feb 2008 23:33:59 +0000"  >&lt;p&gt;In theory, writing directly to HDFS would be the fastest way to import data. However, the tricky part in my mind is that you need all the partitions not just to be sorted internally but sorted amongst each other. This means that the partitioning function you use has to be able to sort lexically as well. Without knowing what the data looks like ahead of time, how can you know how to efficiently partition the data into regions?&lt;/p&gt;

&lt;p&gt;This doesn&apos;t account for trying to import a lot of data into a new table. In that case, it&apos;d be quite futile to write tons of data into the existing regions range, because that would just cause the existing regions would just become enormous, and then all you&apos;re really doing is putting off the speed hit until the split/compact stage.&lt;/p&gt;

&lt;p&gt;What is it that actually holds back the speed of imports? The API mechanics and nothing else? The number of region servers participating in the import? The speed of the underlying disk? Do we even have a sense of what would be a good speed for bulk imports in the first place? I think this issue needs better definition before we can say what we should do.&lt;/p&gt;</comment>
                            <comment id="12566400" author="stack" created="Wed, 6 Feb 2008 23:49:28 +0000"  >&lt;p&gt;If a new table, make a region per reducer (Configure many reducers if table is big).  The framework will have done the sorting (lexigraphically if thats our key compare function) for us  (Might have to add to the framework to ensure we don&apos;t split a key in the middle of a row).&lt;/p&gt;

&lt;p&gt;If a table that already exists, would be reducer per existing region and yeah, there&apos;ll be a splitting and compacting price to pay.&lt;/p&gt;

&lt;p&gt;To see difference in speeds going via API versus writing direct to mapfiles, see primitive PerformanceEvaluation and compare numbers writing mapfiles directly rather than going API.&lt;/p&gt;</comment>
                            <comment id="12613402" author="jdcryans" created="Mon, 14 Jul 2008 18:29:41 +0000"  >&lt;p&gt;Something HBase should have is a BatchUpdate that takes multiple row keys. A simple version of it would be doing many BatchUpdate like we already have but in an iteration. An enhanced version would instead do something like this when there is only a few regions :&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Sort the row keys&lt;/li&gt;
	&lt;li&gt;Sample some rows to get an average row size&lt;/li&gt;
	&lt;li&gt;Using the existing region(s) with the row keys to insert and the average row size, figure how the splits would be done&lt;/li&gt;
	&lt;li&gt;Insert the missing rows that would be the new lows and highs&lt;/li&gt;
	&lt;li&gt;Force the desired splits&lt;/li&gt;
	&lt;li&gt;Insert remaining data&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12617222" author="stack" created="Sat, 26 Jul 2008 22:19:17 +0000"  >&lt;p&gt;Thinking more on this issue, in particular on Billy&apos;s suggestion above (&apos;Billy Pearson - 06/Feb/08 01:07 PM&apos;), bulk uploading by writing store files ain&apos;t hard:&lt;/p&gt;

&lt;p&gt;For a new table (as per Bryan above), its particularly easy.  Do something like:&lt;/p&gt;

&lt;p&gt;+ Create table in hbase&lt;br/&gt;
+ Mark it read-only or disabled even.&lt;br/&gt;
+ Start mapreduce job.  In its configuration would go to the master to read the table description.&lt;br/&gt;
+ Map reads whatever the input using whatever formatter and ouputs from the map using HStoreKey for key and cell content for value.&lt;br/&gt;
+ Job would use fancy new TableFileReduce.  Each reducer would write a region.  It&apos;d know what for start and end keys &amp;#8211; they&apos;d be the first and last it&apos;d see.  Could output these somewhere so a tail task could find them.  The file outputter would need to also do sequenceids of some form.&lt;br/&gt;
+ When job was done, tail task would insert regions into meta using MetaUtils.&lt;br/&gt;
+ Enable the table.&lt;br/&gt;
+ If regions are lop-sided, hbase will do the fixup.&lt;/p&gt;

&lt;p&gt;If table already exists:&lt;/p&gt;

&lt;p&gt;+ Mark table read-only (ensure this prevents splits and that it means memcache is flushed)&lt;br/&gt;
+ Start a mapreduce job that read from master the table schema and its regions (and the master&apos;s current time so we don&apos;t write records older).&lt;br/&gt;
+ Map as above.&lt;br/&gt;
+ Reducer as above only insert smarter partitioner, one that respected region boundaries and that made a reducer per current region.&lt;br/&gt;
+ Enable hbase and let it fix up where storefiles written were too big by splitting etc.&lt;/p&gt;

&lt;p&gt;It don&apos;t seem hard at all to do.&lt;/p&gt;</comment>
                            <comment id="12617233" author="viper799" created="Sun, 27 Jul 2008 03:01:47 +0000"  >&lt;p&gt;With a read-only option that would simplify things a lot. Do we have this yet I remember reading something about adding it in the past.&lt;/p&gt;</comment>
                            <comment id="12617239" author="stack" created="Sun, 27 Jul 2008 04:17:39 +0000"  >&lt;p&gt;We do.  Went in as part of Andrew Purtell&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-62&quot; title=&quot;[hbase] Allow user add arbitrary key/value pairs to table and column descriptors&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-62&quot;&gt;&lt;del&gt;HBASE-62&lt;/del&gt;&lt;/a&gt; work.  There&apos;s even a unit test!&lt;/p&gt;</comment>
                            <comment id="12621331" author="viper799" created="Mon, 11 Aug 2008 01:19:31 +0000"  >&lt;p&gt;Been thanking on this one what happens if a region get to many map files written to it and there to large to compaction and we OOME because of lack memory to compact the files or will this not happen?&lt;/p&gt;</comment>
                            <comment id="12621521" author="stack" created="Mon, 11 Aug 2008 17:49:24 +0000"  >&lt;p&gt;Lets test Billy and make sure that doesn&apos;t happen.&lt;/p&gt;</comment>
                            <comment id="12737728" author="stack" created="Fri, 31 Jul 2009 21:50:11 +0000"  >&lt;p&gt;Here is a patch to add to classes to MapReduce: KeyValueSortReducer and HFileOutputFormat.  This patch also adds a small test class that runs a MR job that has custom mapper and inputformat.  The inputformat produces PerformanceEvaluation type keys and values (keys are a zero-padded long and values are random 1k of bytes).  The mapper takes this inputformat and outputs the key as row and then makes a KeyValue of the row, a defined column and the value.&lt;/p&gt;

&lt;p&gt;KeyValueSortReducer takes as input an ImmutableBytesWritable as key/row.  It then pulls on the Iterator to read in all of the passed KeyValues, sorts then, and then starts outputting the sorted key/row+KeyValue.&lt;/p&gt;

&lt;p&gt;HFileOutputFormat takes ImmutableBytesWritable and KeyValue. On setup, it reads configuration for stuff like blocksize and compression to use.  It then writes HFiles of &amp;lt; hbase.hregion.max.filesize size.&lt;/p&gt;

&lt;p&gt;Next I&apos;ll work on a script that takes an HTableDescriptor and some other parameters and that then puts the output of this MR into proper layout in HDFS with an hfile per region making proper insertions into catalog tables.&lt;/p&gt;</comment>
                            <comment id="12737773" author="stack" created="Fri, 31 Jul 2009 23:37:33 +0000"  >&lt;p&gt;Have HFileOutputFormat handle multiple families.  Reducer can pass any number of families and HFO will write hfiles to family named subdirs.&lt;/p&gt;</comment>
                            <comment id="12737776" author="stack" created="Fri, 31 Jul 2009 23:50:13 +0000"  >&lt;p&gt;Start of a script that will move hfiles into place under hbase.rootdir and that then updates catalog table.  Not finished yet.&lt;/p&gt;</comment>
                            <comment id="12737833" author="stack" created="Sat, 1 Aug 2009 04:37:34 +0000"  >&lt;p&gt;More fixup.  Add the script to the patch.  Turns out, multiple families is a bit more complicated.  Can do that later if wanted.&lt;/p&gt;</comment>
                            <comment id="12737972" author="stack" created="Sun, 2 Aug 2009 01:02:28 +0000"  >&lt;p&gt;This patch seems to basically work.  I took files made by the TestHFileInputFormat test and passed them to the script as follows:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$  ./bin/hbase org.jruby.Main bin/loadtable.rb xyz /tmp/testWritingPEData/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The script expects hbase to be running.&lt;/p&gt;

&lt;p&gt;It ran through the list of hfiles, read their meta info and last key.  It then sorted the hfiles by end key.  It makes a HTableDescriptor and HColumnDescriptor with defaults (If want other than defaults, then after upload, alter table).  It then takes the sorted files and per file moves it into place and adds a row to .META.  Doesn&apos;t take long.&lt;/p&gt;

&lt;p&gt;The meta scanner runs after the upload and deploys the regions.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;I&apos;ll not work on this anymore, not till someone else wants to try it.&lt;/p&gt;</comment>
                            <comment id="12737978" author="stack" created="Sun, 2 Aug 2009 02:48:21 +0000"  >&lt;p&gt;I deleted my last comment.  It was duplication of stuff said earlier in this issue a good while ago.&lt;/p&gt;

&lt;p&gt;I changed title of this issue to only be about bulk upload.   The bulk dump is going on elsewhere: e.g. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1684&quot; title=&quot;Backup (Export/Import) contrib tool for 0.20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1684&quot;&gt;&lt;del&gt;HBASE-1684&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On earlier comments about splitting table so at least a region per regionserver, that ain&apos;t hard to do now.  You can do it via UI &amp;#8211; force a split &amp;#8211; or just write a little script to add a table and initial region range (for example, see the script in the attached patch).&lt;/p&gt;

&lt;p&gt;I think criteria for closing this issue is commit of some set of tools that allow writing hfiles either into new tables or into extant tables.&lt;/p&gt;</comment>
                            <comment id="12743333" author="stack" created="Fri, 14 Aug 2009 18:13:02 +0000"  >&lt;p&gt;Two things.&lt;/p&gt;

&lt;p&gt;1. Manish Shah points out that we need to ensure a total ordering of all keys (See pg.218 of hadoop book and on).  We need to supply a partitioner that does total ordering AND that ensures we don&apos;t partition across rows (Thanks for this Manish).&lt;/p&gt;

&lt;p&gt;2. Current patch does not guarantee rows do not span storefiles.&lt;/p&gt;</comment>
                            <comment id="12743335" author="stack" created="Fri, 14 Aug 2009 18:16:10 +0000"  >&lt;p&gt;Oh, and 3., to do multiple families shouldn&apos;t be hard; just rotate all files when one is in excess of maximum (on a row boundary)&lt;/p&gt;</comment>
                            <comment id="12744758" author="stack" created="Tue, 18 Aug 2009 22:34:56 +0000"  >&lt;p&gt;Updated patch.  Rolls hfile now at row boundary.&lt;/p&gt;

&lt;p&gt;Regards TotalOrderPartitioner, there is no such facility in the new mapreduce package.   That said, shouldn&apos;t be too hard making a partitioner of our own.  Here is the default hash partitioner:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /** Use {@link &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;#hashCode()} to partition. */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getPartition(K key, V value,
                          &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; numReduceTasks) {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (key.hashCode() &amp;amp; &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE) % numReduceTasks;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We could take as inputs a start and end key and then divide the key space using our key bigdecimal math into numReduceTasks partitions?&lt;/p&gt;</comment>
                            <comment id="12753820" author="streamy" created="Thu, 10 Sep 2009 21:13:09 +0000"  >&lt;p&gt;v5 patch does not include ruby script.  Posted ruby script is incomplete.  Working one was in v4 patch.&lt;/p&gt;

&lt;p&gt;Will attach new patch shortly.&lt;/p&gt;</comment>
                            <comment id="12757186" author="streamy" created="Fri, 18 Sep 2009 14:50:28 +0000"  >&lt;p&gt;The MR job is working tremendously well for me.  I&apos;m able to almost instantly saturate my entire cluster during an upload and it remains saturated until the end.  Full CPU usage, lots of io-wait, so I&apos;m disk io-bound as I should be.&lt;/p&gt;

&lt;p&gt;I did a few runs of a job which imported between 1M and 10M rows, each row containing a random number of columns from 1 to 1000.  In the end, I imported between 500M and 5B KeyValues.&lt;/p&gt;

&lt;p&gt;On a 5 node cluster of 2core/2gb/250gb nodes, I could import 1M rows / 500M keys in 7.5 minutes (2.2k rows/sec, 1.1M keys/sec).&lt;/p&gt;

&lt;p&gt;On a 10 node cluster of 4core/4gb/500gb nodes, I could do the same import in 2.5 minutes.  On this larger cluster I also ran the same job but with 10M rows / 5B keys in 25 minutes (6.6k rows/sec, 3.3M keys/sec).&lt;/p&gt;

&lt;p&gt;Previously running HTable-based imports on these clusters, I was seeing between 100k and 200k keys/sec, so this represents a 5-15X speed improvement.  In addition, the imports finish without any problem (I would have killed the little cluster running these imports through HBase).&lt;/p&gt;


&lt;p&gt;I think there is a bug with the ruby script though.  It worked sometimes, but other times it ended up hosing the cluster until I restarted.  Things worked fine after restart.&lt;/p&gt;

&lt;p&gt;Still digging...&lt;/p&gt;</comment>
                            <comment id="12758755" author="streamy" created="Wed, 23 Sep 2009 16:24:27 +0000"  >&lt;p&gt;I&apos;ve got this working.  Running final tests on branch and trunk and will post patch.&lt;/p&gt;</comment>
                            <comment id="12758760" author="streamy" created="Wed, 23 Sep 2009 16:36:53 +0000"  >&lt;p&gt;Patch that applies cleanly to branch.&lt;/p&gt;

&lt;p&gt;Includes two modifications to the ruby script.  Ignores _log directories (preventing multiple-family error) and copies hfiles into proper region directories, after creating them.&lt;/p&gt;

&lt;p&gt;Running final test now.&lt;/p&gt;</comment>
                            <comment id="12758784" author="streamy" created="Wed, 23 Sep 2009 17:13:03 +0000"  >&lt;p&gt;v6 patch works as advertised.  &lt;/p&gt;

&lt;p&gt;Just imported 200k rows with avg of 500 columns each for 100M total KVs (24 regions).  MR job ran in under 2 minutes into a 4 node cluster of 2core/2gb/250gb nodes.  Ruby script takes 3-4 seconds and then about 30 seconds for cluster to assign out regions.&lt;/p&gt;

&lt;p&gt;I&apos;m ready to commit this to trunk and branch though we need some docs.  Will open separate JIRA for multi-family support.&lt;/p&gt;</comment>
                            <comment id="12758820" author="stack" created="Wed, 23 Sep 2009 18:36:39 +0000"  >&lt;p&gt;This version adds doc to the mapreduce package-info explaining how bulk import works.&lt;/p&gt;</comment>
                            <comment id="12758823" author="stack" created="Wed, 23 Sep 2009 18:43:40 +0000"  >&lt;p&gt;Committed last patch to trunk and branch (jgray reviewed and tested).  Opening new issue for multi-family bulk import.&lt;/p&gt;</comment>
                            <comment id="15017779" author="lars_francke" created="Fri, 20 Nov 2015 13:01:21 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12465924">HIVE-1383</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="12436435">HBASE-1861</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12415198" name="48-v2.patch" size="16935" author="stack" created="Fri, 31 Jul 2009 23:37:33 +0000"/>
                            <attachment id="12415222" name="48-v3.patch" size="21620" author="stack" created="Sat, 1 Aug 2009 04:37:34 +0000"/>
                            <attachment id="12415256" name="48-v4.patch" size="23516" author="stack" created="Sun, 2 Aug 2009 01:02:28 +0000"/>
                            <attachment id="12416926" name="48-v5.patch" size="17871" author="stack" created="Tue, 18 Aug 2009 22:34:56 +0000"/>
                            <attachment id="12420397" name="48-v7.patch" size="27214" author="stack" created="Wed, 23 Sep 2009 18:36:39 +0000"/>
                            <attachment id="12415186" name="48.patch" size="15715" author="stack" created="Fri, 31 Jul 2009 21:50:11 +0000"/>
                            <attachment id="12420381" name="HBASE-48-v6-branch.patch" size="22917" author="streamy" created="Wed, 23 Sep 2009 16:36:53 +0000"/>
                            <attachment id="12415200" name="loadtable.rb" size="3627" author="stack" created="Fri, 31 Jul 2009 23:50:13 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 8 Dec 2007 06:14:55 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31648</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h3t3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>97879</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>