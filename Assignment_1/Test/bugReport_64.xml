<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:40:40 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-64/HBASE-64.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-64] Add max number of mapfiles to compact at one time giveing us a minor &amp; major compaction</title>
                <link>https://issues.apache.org/jira/browse/HBASE-64</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently we do compaction on a region when the hbase.hstore.compactionThreshold is reached - default 3&lt;/p&gt;

&lt;p&gt;I thank we should configure a max number of mapfiles to compact at one time simulator to doing a minor compaction in bigtable. This keep compaction&apos;s form getting tied up in one region to long letting other regions get way to many memcache flushes making compaction take longer and longer for each region&lt;/p&gt;

&lt;p&gt;If we did that when a regions updates start to slack off the max number will eventuly include all mapfiles causeing a major compaction on that region. Unlike big table this would leave the master out of the process and letting the region server handle the major compaction when it has time.&lt;/p&gt;

&lt;p&gt;When doing a minor compaction on a few files I thank we should compact the newest mapfiles first leave the larger/older ones for when we have low updates to a region.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12386311">HBASE-64</key>
            <summary>Add max number of mapfiles to compact at one time giveing us a minor &amp; major compaction</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="viper799">Billy Pearson</reporter>
                        <labels>
                    </labels>
                <created>Tue, 15 Jan 2008 21:50:53 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:48 +0000</updated>
                            <resolved>Fri, 15 Aug 2008 23:42:10 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12560096" author="viper799" created="Thu, 17 Jan 2008 21:53:36 +0000"  >&lt;p&gt;I thank we need to watch and make sure the split check is not launched on a minor compaction.&lt;/p&gt;

&lt;p&gt;Example:&lt;br/&gt;
Say we have a new region fresh split and it starts getting heave updates and memcache flushes. If we have not compacted the old hstors form the original split then it would not likely to be wise to split again.&lt;/p&gt;</comment>
                            <comment id="12561871" author="jimk" created="Wed, 23 Jan 2008 23:36:15 +0000"  >&lt;p&gt;Blocking this issue to see if &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-69&quot; title=&quot;[hbase] Make cache flush triggering less simplistic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-69&quot;&gt;&lt;del&gt;HADOOP-2636&lt;/del&gt;&lt;/a&gt; resolves the issue&lt;/p&gt;</comment>
                            <comment id="12562335" author="stack" created="Fri, 25 Jan 2008 01:47:30 +0000"  >&lt;p&gt;Doing some load testing, I&apos;m seeing compactions taking longer and longer as reported above but I&apos;m also seeing that the region won&apos;t split.   Just goes from one compaction to the next w/ each doing more and more files taking longer each time.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-01-25 01:16:45,850 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 55sec
2008-01-25 01:18:37,347 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 1mins, 49sec
2008-01-25 01:21:42,010 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 3mins, 4sec
2008-01-25 01:27:20,417 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 5mins, 38sec
2008-01-25 01:37:55,330 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 10mins, 34sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looking more into this.&lt;/p&gt;</comment>
                            <comment id="12562361" author="stack" created="Fri, 25 Jan 2008 05:11:14 +0000"  >&lt;p&gt;Here&apos;s the next two compactions:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-01-25 01:58:30,280 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 20mins, 34sec
2008-01-25 02:17:28,818 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region TestTable,,1201223677355. Took 18mins, 58sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The split final runs, 15 minutes after the loading finishes and an hour after its initially scheduled.  Split is frozen out waiting here in HRegion:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (writestate.compacting || writestate.flushing) {
          LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;&quot;&lt;/span&gt; +
              (writestate.compacting ? &lt;span class=&quot;code-quote&quot;&gt;&quot; compaction&quot;&lt;/span&gt; : &quot;&quot;) +
              (writestate.flushing ?
                  (writestate.compacting ? &lt;span class=&quot;code-quote&quot;&gt;&quot;,&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;) + &quot;&lt;/span&gt; cache flush&quot; :
                    &quot;&quot;
              ) + &lt;span class=&quot;code-quote&quot;&gt;&quot; to complete &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region &quot;&lt;/span&gt; + regionName
          );
          &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            writestate.wait();
          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException iex) {
            &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;
&lt;/span&gt;          }
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12562367" author="stack" created="Fri, 25 Jan 2008 05:36:10 +0000"  >&lt;p&gt;Patch that only compacts a maximum of twice the compaction threshold.  Testing to see if split thread gets a chance to run when we have upper bound on number of files to compact per invocation &lt;/p&gt;</comment>
                            <comment id="12562374" author="viper799" created="Fri, 25 Jan 2008 06:26:40 +0000"  >&lt;p&gt;Thats what I see too the split never happens when a region is under load of inserts. I still thank if we are going to have transactions speed close to bigtables we will need to add a limit on number of map files to compaction at one time. &lt;br/&gt;
Even if &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-69&quot; title=&quot;[hbase] Make cache flush triggering less simplistic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-69&quot;&gt;&lt;del&gt;HADOOP-2636&lt;/del&gt;&lt;/a&gt; get the flushing working right for performance point of view I thank it should be included as any ways to handle large number of regions per server. &lt;/p&gt;

&lt;p&gt;I am seeing 10-15 mins to run compaction on a 90MB region using block compression. &lt;br/&gt;
So if you consider that most will want to handed more then 25-50 regions per server.&lt;/p&gt;

&lt;p&gt;Say avg region server holds 100 regions thats going to work out to be 100*10mins = 1000 mins = 16hours to run a full compaction on all the regions.&lt;br/&gt;
By havening this in place on regions getting large update traffic the map files will not get out of control.&lt;/p&gt;

&lt;p&gt;100 regions with 90MB avg size only equals about 9GB of compressed data.&lt;br/&gt;
I would like to see closer to production release better compression method used. &lt;br/&gt;
This would help with compaction speed right now my bottle neck on compaction is compression.&lt;/p&gt;

{New Idea}
&lt;p&gt;After thinking on this a little not sure doing a compaction on the number of map files it the best way to go.&lt;br/&gt;
Compaction on 3-6 small 1-2mb map files does not take that long even with compression so the idea way to do this would be to only &lt;br/&gt;
compaction small files while we have small files to compaction leaving more larger map files to compact in the end when load is as high.&lt;/p&gt;

&lt;p&gt;big tables has the right idea only do a full/major compaction of all the map files every so often to remove deleted data or data out of its max version range.&lt;br/&gt;
so we might want to look at the idea of removing the compaction based on the number of map files to a limit on the size of the map files&lt;br/&gt;
example say we have a region family compaction max size 16MB we could only compact files under that size once we compact regions that total more then the &lt;br/&gt;
max compaction size then do not include that map file in the next compaction. This would leave map files around the same size to be compacted together say once a day and/or after splits.&lt;br/&gt;
also I would like to keep the region server handle the compaction on there own so the master can be left alone to do other more important task.&lt;/p&gt;

&lt;p&gt;Currently if you load a region server with many regions it always be running compaction&apos;s on the regions if there getting data inserted.&lt;br/&gt;
So this would lesses the load on the hard drives, memory, and cpus giving more resources for faster/more  transactions.&lt;/p&gt;</comment>
                            <comment id="12562375" author="stack" created="Fri, 25 Jan 2008 06:28:25 +0000"  >&lt;p&gt;This is a better patch. Moves the setting of writesEnabled before we go into the wait.  But its still not right.  We start to split but writesEnabled also stops flushing so memcache fills, the block updates gate comes down, clients timout.  Need a flag that stops another compaction running but lets flushes happen.&lt;/p&gt;</comment>
                            <comment id="12562593" author="stack" created="Fri, 25 Jan 2008 17:35:47 +0000"  >&lt;p&gt;Add a new disable compactions.  Gets set when trying to close a region so no more compactions can be scheduled.  Tested under load and splits happen again.&lt;/p&gt;</comment>
                            <comment id="12562709" author="stack" created="Fri, 25 Jan 2008 22:02:14 +0000"  >&lt;p&gt;I made issue &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-247&quot; title=&quot;[hbase] under load, regions won&amp;#39;t split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-247&quot;&gt;&lt;del&gt;HADOOP-2712&lt;/del&gt;&lt;/a&gt; to cover not-splitting under load.&lt;/p&gt;

&lt;p&gt;Billy, in bigtable paper, I believe what we call a flush is a minor compaction in gwhogle-speak and a merging compaction is what they call compaction of a few store files interleaving whats in memcache.&lt;/p&gt;

&lt;p&gt;.bq When doing a minor compaction on a few files I thank we should compact the newest mapfiles first leave the larger/older ones for when we have low updates to a region.&lt;/p&gt;

&lt;p&gt;Why you think newer rather than older Billy?&lt;/p&gt;

&lt;p&gt;.bq I still thank if we are going to have transactions speed close to bigtables we will need to add a limit on number of map files to compaction at one time.&lt;/p&gt;

&lt;p&gt;I agree given the times to compact posted above.&lt;/p&gt;

&lt;p&gt;By the way, I tried out my simple upper-bound patch that put a cap of 2*compactionThreshold on number of files to compact at once.  Seems to work with messages like below showing from time to time:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2008-01-25 20:44:38,330 DEBUG org.apache.hadoop.hbase.HStore: Count of files to compact in 2052803679/info is 8 which is &amp;gt; twice compaction threshold of 3. Compacting 6 only
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FYI, regionserver runs compaction.  Master has no say at moment.&lt;/p&gt;


</comment>
                            <comment id="12562821" author="viper799" created="Sat, 26 Jan 2008 05:37:59 +0000"  >&lt;p&gt;Example for high number of updates on hot regions&lt;/p&gt;

&lt;p&gt;Say I have many regions say 100 on a server and a few are getting a lot of updates and the others are getting some updates. &lt;/p&gt;

&lt;p&gt;The few that are getting the bulk of the updates will have many map files causing the compaction to limit to the 6 map file limit but will be quick to finish sense we will only be working with say 16-20MB not 64MB+. &lt;br/&gt;
If we go from new to old leaveing out the oldest map files that are the largest and take the longest to include in compaction.&lt;/p&gt;

&lt;p&gt;The other regions will still tie up the compaction thread for say 10 mins each on regions that only has 3-4 map files becuase it will include the larger map files for compaction.&lt;br/&gt;
In that time the two region that are getting lots of  updates will be flushing more often meaning they will have many map files. &lt;br/&gt;
We will be spending most of our time compaction region that have only a few map files including the larger map files that take the longest our time to compact instead of on the region that have the most map files to compact.&lt;/p&gt;

&lt;p&gt;In my example above if all/most the regions flushed a map file and entered in to the que for compaction it would be 16 hours before we got back to the &lt;br/&gt;
few regions that had been getting the bulk of the updates. Then when we got back to them we would only be processing 6 of the map files again then leavening many map files for the next compaction&lt;br/&gt;
looping and doing all the others again assuming they will get a few flushes over the 16 hours it took to complete the cmopaction on all the regions.&lt;/p&gt;

&lt;p&gt;We should try to come up with a simple test outside of Hudson to see real numbers on the time it takes to  do a scan on a region say run the test with 10,20,100,500 map files.&lt;/p&gt;

&lt;p&gt;With my new idea above we could keep the number of map files under control by only running compaction map files under X size keeping the compaction fast. the test many &lt;br/&gt;
This may show that we can handle say 50 medium size map files during a scan with out much impact on speed. if this is the case then we may not have to do major compaction&apos;s where we merge all the map files together but once every few days. Except on a split then we would want to do a major compaction soon to remove the out of range data from each new region.&lt;/p&gt;

&lt;p&gt;The bottle neck I have seen on compaction is with block compress we am bound by the cpu speed to gzip the map file after compaction. So I would rather run one large compaction once a day or two and have to gzip the biggest part of  each region only every few days in place of doing every day or more then once a day. In my mind thats wasting resources gunzip 64MB data add 4MB gzip it. and do this many times a day I thank thats wasting cpu time on gziping the same data over and over again&lt;/p&gt;

&lt;p&gt;My idea here is to spend more time on compaction on regions getting more updates then the other regions so we can handle more regions per server.&lt;br/&gt;
Currently my example above is based on 100 regions totaling 9GB of compressed data. With that kind of number per server someone wanting to store a TB of compressed data in hbase they would need a vary large number of servers or have low update traffic.&lt;/p&gt;

&lt;p&gt;I know we have some other issues on how many regions a server can handle with the open files limits per server and stuff like that but I would like to see this compaction problem fix once and have the most efficient compaction we can for all users and removing it from becoming a issue later down the road. In the end if we go with this new idea it would mean that the compaction&apos;s would be faster and use less resources during bulk updates and allow more resources to other task running on the server like map task.&lt;/p&gt;

&lt;p&gt;So my proposed idea would be to have two types of compaction&apos;s&lt;/p&gt;

&lt;p&gt;1. Compact new flushes in to one map files until it reach a size in MB&apos;s then leave it for compaction below&lt;br/&gt;
2. Compact all map files for a region to together once every x days or if we are child region from a split.&lt;/p&gt;</comment>
                            <comment id="12562823" author="viper799" created="Sat, 26 Jan 2008 05:55:42 +0000"  >&lt;p&gt;noting I am not thanking of havening two compaction threads just two options a compaction can take we still que up a compaction check on a memcache flush but&lt;br/&gt;
run compaction #2 above if oldest map file is &amp;gt; X days else check to see if there are 2 or more new map files less then the max compaction MB set and run #1 if true.&lt;/p&gt;</comment>
                            <comment id="12563803" author="bryanduxbury" created="Wed, 30 Jan 2008 02:20:34 +0000"  >&lt;p&gt;It appears these two issues are probably related. I actually thought that this one had been fixed and committed already, but I guess not.&lt;/p&gt;</comment>
                            <comment id="12565213" author="viper799" created="Sun, 3 Feb 2008 20:42:12 +0000"  >&lt;p&gt;I got a second ideal on this to help with hot spots&lt;/p&gt;

&lt;p&gt;If we could add a way to set a  priority for compaction&apos;s this would help with the hot spots regions building up to many map files flushes.&lt;/p&gt;

&lt;p&gt;Example if we have a region with 25 map files and one with 10&lt;/p&gt;

&lt;p&gt;Region with 25 map files would have a priority of 25&lt;br/&gt;
and the one with 10 map files have a priority of 10 we would compact the region with 25 before 10&lt;/p&gt;

&lt;p&gt;If we could add/update the priority when we do a flush then the compactor could work on region that need it the most in order.&lt;/p&gt;</comment>
                            <comment id="12622050" author="stack" created="Wed, 13 Aug 2008 00:41:46 +0000"  >&lt;p&gt;There is a bunch of good stuff above but its hard to tell which still applies since &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-745&quot; title=&quot;scaling of one regionserver, improving memory and cpu usage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-745&quot;&gt;&lt;del&gt;HBASE-745&lt;/del&gt;&lt;/a&gt; went in.  There is also some overlap with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-775&quot; title=&quot;max mapfiles to compact at one time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-775&quot;&gt;&lt;del&gt;HBASE-775&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12622323" author="viper799" created="Wed, 13 Aug 2008 19:22:51 +0000"  >&lt;p&gt;I thank &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-745&quot; title=&quot;scaling of one regionserver, improving memory and cpu usage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-745&quot;&gt;&lt;del&gt;HBASE-745&lt;/del&gt;&lt;/a&gt; did basically what I was looking for above when &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-775&quot; title=&quot;max mapfiles to compact at one time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-775&quot;&gt;&lt;del&gt;HBASE-775&lt;/del&gt;&lt;/a&gt; is resolved this issue should be closed.&lt;/p&gt;

&lt;p&gt;the only other item above or I had idea of was changeing the order of compaction to the one with the most map files first.&lt;br/&gt;
With the speed gain from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-745&quot; title=&quot;scaling of one regionserver, improving memory and cpu usage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-745&quot;&gt;&lt;del&gt;HBASE-745&lt;/del&gt;&lt;/a&gt; I do not thank that is needed even if we had 100 regions per server compaction would keep up now I thank.&lt;/p&gt;

&lt;p&gt;So unless anyone else is seeing backlogs in the compaction&apos;s now I thank the compaction&apos;s bottlenecks have been solved.&lt;/p&gt;</comment>
                            <comment id="12623060" author="stack" created="Fri, 15 Aug 2008 23:42:10 +0000"  >&lt;p&gt;Opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-834&quot; title=&quot;&amp;#39;Major&amp;#39; compactions and upper bound on files we compact at any one time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-834&quot;&gt;&lt;del&gt;HBASE-834&lt;/del&gt;&lt;/a&gt; to cover original intent of this issue.  Closing this one since it got pulled all around the place.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12386455">HBASE-69</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12387412">HBASE-138</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12374064" name="flag-v2.patch" size="3625" author="stack" created="Fri, 25 Jan 2008 17:35:47 +0000"/>
                            <attachment id="12374007" name="flag.patch" size="2128" author="stack" created="Fri, 25 Jan 2008 06:28:25 +0000"/>
                            <attachment id="12374006" name="twice.patch" size="1182" author="stack" created="Fri, 25 Jan 2008 05:36:10 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 23 Jan 2008 23:36:15 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31661</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 18 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h3zb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>97907</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>